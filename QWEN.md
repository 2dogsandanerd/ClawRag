# ClawRAG - The Brain for OpenClaw

## ProjektÃ¼bersicht

ClawRAG ist ein produktionsbereites, selbst gehostetes RAG-Engine (Retrieval Augmented Generation), das als langfristiges GedÃ¤chtnis ("The Brain") fÃ¼r autonome Agenten wie OpenClaw fungiert. Das Projekt extrahiert essentielle kognitive Funktionen aus dem Enterprise-Core und bietet eine robuste API fÃ¼r Dokumenten-Ingestion und Abfrage. Die Architektur trennt Intelligenz von Aktionen und ermÃ¶glicht skalierbare KI-Anwendungen.

### Hauptmerkmale

- **Modernes Dokumenten-Processing**: Nutzt Docling 2.13.0 fÃ¼r PDF, DOCX, PPTX, XLSX, HTML, Markdown
- **Hybride Suche**: VektorÃ¤hnlichkeit + BM25 SchlÃ¼sselwortsuche mit Reciprocal Rank Fusion
- **ChromaDB 0.5.23**: Vektorspeicher mit Verbindungspooling und GesundheitsprÃ¼fungen
- **LlamaIndex 0.12.9**: Fortgeschrittene Abruf-Pipelines
- **Multi-LLM-UnterstÃ¼tzung**: Ollama (Standard), OpenAI, Anthropic, Gemini
- **Leichtgewichtige UI**: Einzelne HTML/JS-Dashboard-Datei mit Live-Systemprotokollen
- **Docker-first**: Produktionsbereite Bereitstellung mit Hot-Reload-UnterstÃ¼tzung
- **MCP-Integration**: Eingebauter Model Context Protocol-Server fÃ¼r OpenClaw
- **Ordner-Ingestion**: Massenverarbeitung von Dokumenten mit Echtzeit-Fortschrittsanzeige

## Technische Architektur

### Hauptkomponenten

- **Backend**: FastAPI-basierte API mit Endpunkten fÃ¼r Ingestion, Abfrage, Sammlungen und Dokumente
- **ChromaDB**: Vektordatenbank fÃ¼r semantische Speicherung und Abruf
- **Ollama**: Lokale LLM- und Embedding-Inferenz
- **Docling**: Modernes Dokumentenverarbeitungs-Framework
- **MCP-Server**: Model Context Protocol-Server fÃ¼r OpenClaw-Integration
- **nginx**: Reverse-Proxy-Gateway fÃ¼r alle Dienste

### Kernmuster

- **Singleton**: ChromaManager fÃ¼r einzelne Verbindungsinstanz
- **Resilienz**: Circuit-Breaker + Wiederholungslogik fÃ¼r ChromaDB
- **Lifespan**: Korrektes FastAPI-Startup/-Shutdown fÃ¼r saubere Verbindungen
- **Hot-Reload**: Quellcode als Volume eingehÃ¤ngt fÃ¼r Entwicklung

### Dateistruktur

```
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/v1/rag/         # RAG-Endpunkte
â”‚   â”‚   â”‚   â”œâ”€â”€ collections.py  # Sammlungs-CRUD
â”‚   â”‚   â”‚   â”œâ”€â”€ documents/      # Upload, Verwaltung
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py        # RAG-Abfragen
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion/      # Ordnerscan, Batch-Verarbeitung
â”‚   â”‚   â”‚   â””â”€â”€ cockpit.py      # Systemstatus
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ chroma_manager.py      # ChromaDB-Singleton
â”‚   â”‚   â”‚   â”œâ”€â”€ docling_loader.py      # Dokumentenparser
â”‚   â”‚   â”‚   â”œâ”€â”€ query_engine.py        # AbfrageausfÃ¼hrung
â”‚   â”‚   â”‚   â”œâ”€â”€ retrievers/            # Hybrid, BM25, Re-Ranker
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py              # Multi-LLM-Konfiguration
â”‚   â”‚   â”‚   â””â”€â”€ feature_limits.py      # Editions-Tarife
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ docling_service.py     # Zentrale Dok-Verarbeitung
â”‚   â”‚       â”œâ”€â”€ classification.py      # Dok-Klassifikation
â”‚   â”‚       â””â”€â”€ generators/            # Zusammenfassungen, Konfigurationen
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ mcp-server/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ server.ts            # MCP-Server-Implementierung
â”‚   â”‚   â”œâ”€â”€ config.ts            # Konfigurationsmanagement
â”‚   â”‚   â””â”€â”€ types.ts             # Typdefinitionen
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md                # MCP-Server-Dokumentation
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html              # Null-Build-Dashboard (Vanilla JS)
â”œâ”€â”€ docker-compose.yml          # VollstÃ¤ndige Stack-Orchestrierung
â”œâ”€â”€ CLAUDE.md                   # Entwicklerhandbuch
â””â”€â”€ README.md
```

## Installation und Betrieb

### Schnellstart (5 Minuten)

#### Voraussetzungen

1. **Docker & Docker Compose** installiert
2. **Ollama** lokal laufend (fÃ¼r Embeddings)
   ```bash
   # Ollama installieren (falls noch nicht installiert)
   curl -fsSL https://ollama.com/install.sh | sh

   # Ollama-Server starten
   ollama serve

   # Embedding-Modell herunterladen (in einem anderen Terminal)
   ollama pull nomic-embed-text
   ```

#### Setup

```bash
# 1. Repository klonen
git clone https://github.com/2dogsandanerd/ClawRag.git
cd ClawRag

# 2. Konfigurieren & Starten
# Interaktives Setup-Skript ausfÃ¼hren, um Ihren Dokumentenordner zu setzen
./setup.sh

# (Alternativ) Manuelles Setup:
# cp .env.example .env
# docker compose up -d

# 4. GesundheitsprÃ¼fung
curl http://localhost:8080/health
# Erwartet: {"status":"healthy","chromadb":"connected","collections_count":0}

# 5. Anwendung Ã¶ffnen
open http://localhost:8080
```

### Dienste (alle Ã¼ber einziges nginx-Gateway):

- Frontend-UI: http://localhost:8080/
- API-Dokumentation: http://localhost:8080/docs
- GesundheitsprÃ¼fung: http://localhost:8080/health
- API-Endpunkte: http://localhost:8080/api/v1/rag/*

## Konfiguration

Umgebungsvariablen werden in der `.env`-Datei konfiguriert:

| Variable | Standard | Beschreibung |
|----------|---------|-------------|
| `LLM_PROVIDER` | `ollama` | LLM-Anbieter (ollama, openai, anthropic, gemini) |
| `LLM_MODEL` | `llama3.1:8b-instruct-q4_k_m` | Modellname fÃ¼r gewÃ¤hlten Anbieter |
| `EMBEDDING_PROVIDER` | `ollama` | Embedding-Anbieter (normalerweise identisch mit LLM) |
| `EMBEDDING_MODEL` | `nomic-embed-text` | Embedding-Modellname |
| `OLLAMA_HOST` | `http://host.docker.internal:11434` | Ollama-Verbindungs-URL |
| `CHROMA_HOST` | `chroma_running` | ChromaDB-Container/Service-Name |
| `CHROMA_PORT` | `8000` | ChromaDB-Port |
| `DOCS_DIR` | `./data/docs` | Host-Verzeichnis, das als `/host_root` fÃ¼r Ordner-Ingestion eingehÃ¤ngt wird |
| `PORT` | `8080` | Externer Port fÃ¼r nginx-Gateway |
| `DEBUG` | `true` | Debug-Protokollierung aktivieren |

## API-Nutzung

### Python-Beispiel

```python
import requests

BASE_URL = "http://localhost:8080/api/v1/rag"

# 1. Eine Sammlung erstellen
response = requests.post(
    f"{BASE_URL}/collections",
    files={
        "collection_name": (None, "mein_wissen"),
        "embedding_provider": (None, "ollama"),
        "embedding_model": (None, "nomic-embed-text")
    }
)
print(f"Sammlung erstellt: {response.json()}")

# 2. Dokumente hochladen
with open("dokument.pdf", "rb") as f:
    response = requests.post(
    f"{BASE_URL}/documents/upload",
    files={"files": f},
    data={
        "collection_name": "mein_wissen",
        "chunk_size": 512,
        "chunk_overlap": 128
    }
)
print(f"Upload-Status: {response.json()}")

# 3. Das Wissensbasis abfragen
response = requests.post(
    f"{BASE_URL}/query",
    json={
        "query": "Was sind die Hauptthemen?",
        "collection": "mein_wissen",
        "k": 5
    }
)
result = response.json()
print(f"Antwort: {result.get('answer')}")
print(f"Quellen: {len(result.get('sources', []))}")
```

### cURL-Beispiele

```bash
# GesundheitsprÃ¼fung
curl http://localhost:8080/health

# Sammlungen auflisten
curl http://localhost:8080/api/v1/rag/collections

# Sammlungsstatistik abrufen
curl http://localhost:8080/api/v1/rag/collections/meine_dokumente/stats

# Abfrage mit spezifischen Parametern
curl -X POST http://localhost:8080/api/v1/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ErklÃ¤re die Architektur",
    "collection": "meine_dokumente",
    "k": 10,
    "similarity_threshold": 0.5
  }'

# Eine Sammlung lÃ¶schen
curl -X DELETE http://localhost:8080/api/v1/rag/collections/meine_dokumente
```

## OpenClaw-Integration via MCP

ClawRAG bietet native UnterstÃ¼tzung fÃ¼r OpenClaw-Integration Ã¼ber das Model Context Protocol (MCP), wodurch Sie direkt aus WhatsApp, Telegram, Discord und anderen OpenClaw-unterstÃ¼tzten KanÃ¤len auf Ihre Wissensbasis zugreifen kÃ¶nnen.

### Installation

1. **ClawRAG starten**:
   ```bash
   docker compose up -d
   ```

2. **MCP-Server in OpenClaw installieren**:
   ```bash
   openclaw mcp add --transport stdio clawrag npx -y @clawrag/mcp-server
   ```

3. **(Optional) Benutzerdefinierten Endpunkt konfigurieren**:
   Falls ClawRAG nicht auf `http://localhost:8080` lÃ¤uft, setzen Sie die Umgebungsvariable:
   ```bash
   export CLAWRAG_API_URL=http://your-clawrag-instance:8080
   ```

### VerfÃ¼gbare MCP-Tools

#### `query_knowledge`
Abfrage Ihrer Wissensbasis aus OpenClaw-KanÃ¤len:

```
Benutzer: "Suche in Wissensbasis nach: Vertragsbedingungen mit Siemens"
OpenClaw: [Ruft query_knowledge(query="Vertragsbedingungen mit Siemens") auf]
ClawRAG: "Laut Siemens_Vertrag_2024.pdf, Seite 12, Abschnitt 4.2: 'Die Haftung ist auf direkte SchÃ¤den bis zu 10% des Vertragswerts begrenzt.' [Zitat: chunk_id: siemens_vertrag_2024_p12_s42]"
```

#### `list_collections`
Entdecken Sie verfÃ¼gbare Wissensbasen:

```
Benutzer: "VerfÃ¼gbare Sammlungen auflisten"
OpenClaw: [Ruft list_collections() auf]
ClawRAG: "VerfÃ¼gbare Sammlungen:
- vertraege (15 Dokumente)
- handbuecher (8 Dokumente)
- protokolle (23 Dokumente)"
```

## Editions-Vergleich

### Community Edition (Dieses Repository)

**Kostenlos & Open Source (Selbst-Hosting)**

- âœ… **Sammlungen: Unbegrenzt**
- âœ… **Dokumente: Unbegrenzt**
- âœ… **Multi-Sammlungssuche-Routing** (Ã¼ber automatisches Fallback)
- âœ… Formate: PDF, Markdown, TXT, DOCX (Ã¼ber Docling)
- âœ… Hybride Suche: Vektor + BM25 (RRF-Fusion)
- âœ… Grundlegende Klassifikation: Heuristik-basiert
- âœ… VollstÃ¤ndiger Zugriff auf Quellcode
- âœ… **MCP-Integration**: Native OpenClaw-Integration Ã¼ber Model Context Protocol
- âŒ Kein Solomon Consensus Engine
- âŒ Kein Mission Cartridge System
- âŒ Keine ML-basierte Missionsanalyse

**Perfekt fÃ¼r:**
- PersÃ¶nliche Wissensbasen
- Interne Firmendokumentation
- Entwicklung und Testing
- VerstÃ¤ndnis der RAG-Architektur
- WhatsApp/Telegram-gesteuerte KI-Assistenten

### Professionelle Edition

- ğŸš€ Sammlungen: 10, jeweils 5000 Dokumente
- ğŸš€ Formate: Erweitert (DOCX, HTML, PPTX, XLSX)
- ğŸš€ Fortgeschrittener Re-Ranking: Cross-Encoder-Modelle
- ğŸš€ Multi-Sammlungssuche: Intelligente Routing
- ğŸš€ ML-Klassifikation: Konfidenzberechnung
- ğŸš€ Analysen & Monitoring
- ğŸš€ Priorisierter Support

### Enterprise Edition

**Die auditierungssichere Wahrheitsschicht**

WÃ¤hrend ClawRAG schnelles, effizientes GedÃ¤chtnis fÃ¼r Agenten bereitstellt, fÃ¼gt die Enterprise Core (V4.0) folgendes hinzu:

- **Solomon Consensus Engine** (Multi-Lane-Validierung Ã¼ber parallele KI-Agenten)
- **Erweitertes Janus** (Mission-basiertes intelligentes Dokumenten-Routing)
- **Surgical HITL** (Visuelle Human-in-the-Loop-Verifizierung mit Bounding Boxes)
- **Citation Enforcer** (Deterministischer Halluzinations-Schutz)
- **Mercator Context Graph** (GraphRAG mit Neo4j-Entity-Extraktion)
- **Six Sigma Laboratory** (Automatische QualitÃ¤tsaudits mit LLM-as-a-Judge)
- **Mission Cartridge System** (Multi-Tenant-Isolation & adaptive Konfiguration)

## Entwicklung

### Lokale Entwicklung (ohne Docker)

```bash
cd backend
pip install -r requirements.txt
uvicorn src.main:app --host 0.0.0.0 --port 8080 --reload
```

**Hinweis:** Sie benÃ¶tigen ChromaDB und Ollama separat laufend.

### Docker-Entwicklung (mit Hot-Reload)

Code-Ã„nderungen werden automatisch erkannt (Quellcode als Volume eingehÃ¤ngt):

```bash
# Code in backend/src/ bearbeiten
# Ã„nderungen werden sofort erkannt, kein Neubau erforderlich

# Protokolle anzeigen
docker compose logs -f backend

# Bei Bedarf neu starten
docker compose restart backend
```

## Lizenz

MIT-Lizenz - Freie Nutzung in kommerziellen und Open-Source-Projekten.

Copyright (c) 2025 2dogsandanerd (ClawRAG Community Edition)

Dieses Produkt enthÃ¤lt Software, die von IBM (Docling) und anderen Open-Source-Mitwirkenden entwickelt wurde.

Docling: https://github.com/DS4SD/docling (MIT-Lizenz)
Copyright (c) 2024 IBM Corp.